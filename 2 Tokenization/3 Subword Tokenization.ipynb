{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2162c615",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Verdana; font-size: 26px; color: magenta\"> 2.3 - Subword Tokenization</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4febe",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Verdana; font-size: 18px; color: darkorange\"> Subword Tokenization is a Natural Language Processing technique(NLP) in which a word is split into subwords and these subwords are known as tokens </p>\n",
    "<p style=\"font-family:Verdana; font-size: 18px; color: darkorange\"> Steps for Subword Tokenization: </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "241e3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaebabb",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Verdana; font-size: 18px; color: magenta\"> Example #1 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42aaec51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original string is : \n",
      "GeeksforGeeks is a fantastic resource for geeks \n",
      "who are looking to enhance their programming skills, \n",
      "and if you're a geek who wants to become an expert programmer, \n",
      "then GeeksforGeeks is definitely the go-to place for geeks like you.\n",
      "\n",
      "The converted string :\n",
      "['geeksforgeeks', 'is', 'a', 'fantastic', 'resource', 'for', 'geeks', 'who', 'are', 'looking', 'to', 'enhance', 'their', 'programming', 'skills', ',', 'and', 'if', 'you', \"'\", 're', 'a', 'geek', 'who', 'wants', 'to', 'become', 'an', 'expert', 'programmer', ',', 'then', 'geeksforgeeks', 'is', 'definitely', 'the', 'go', '-', 'to', 'place', 'for', 'geeks', 'like', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test_str = \"\"\"\n",
    "GeeksforGeeks is a fantastic resource for geeks \n",
    "who are looking to enhance their programming skills, \n",
    "and if you're a geek who wants to become an expert programmer, \n",
    "then GeeksforGeeks is definitely the go-to place for geeks like you.\n",
    "\"\"\"\n",
    "# printing original String\n",
    "print(\"The original string is : \" + str(test_str))\n",
    "test_str=test_str.lower()\n",
    "# using findall() to get all regex matches.\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', test_str)\n",
    "\n",
    "# printing result\n",
    "print(\"The converted string :\\n\" + str(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1011967e",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Verdana; font-size: 18px; color: magenta\"> To get rid of this problem we use tokenization on characters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3e0b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('g e e k s f o r g e e k s', 2),\n",
       "             ('i s', 2),\n",
       "             ('a', 2),\n",
       "             ('f a n t a s t i c', 1),\n",
       "             ('r e s o u r c e', 1),\n",
       "             ('f o r', 2),\n",
       "             ('g e e k s', 2),\n",
       "             ('w h o', 2),\n",
       "             ('a r e', 1),\n",
       "             ('l o o k i n g', 1),\n",
       "             ('t o', 3),\n",
       "             ('e n h a n c e', 1),\n",
       "             ('t h e i r', 1),\n",
       "             ('p r o g r a m m i n g', 1),\n",
       "             ('s k i l l s', 1),\n",
       "             (',', 2),\n",
       "             ('a n d', 1),\n",
       "             ('i f', 1),\n",
       "             ('y o u', 2),\n",
       "             (\"'\", 1),\n",
       "             ('r e', 1),\n",
       "             ('g e e k', 1),\n",
       "             ('w a n t s', 1),\n",
       "             ('b e c o m e', 1),\n",
       "             ('a n', 1),\n",
       "             ('e x p e r t', 1),\n",
       "             ('p r o g r a m m e r', 1),\n",
       "             ('t h e n', 1),\n",
       "             ('d e f i n i t e l y', 1),\n",
       "             ('t h e', 1),\n",
       "             ('g o', 1),\n",
       "             ('-', 1),\n",
       "             ('p l a c e', 1),\n",
       "             ('l i k e', 1),\n",
       "             ('.', 1)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "res_dict=OrderedDict()\n",
    "for i in res:\n",
    "    new_string=' '.join(char for char in i)\n",
    "    if new_string in res_dict:\n",
    "        res_dict[new_string]+=1\n",
    "    else:\n",
    "        res_dict[new_string]=1\n",
    "res_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ece6fb",
   "metadata": {},
   "source": [
    "<p style=\"font-family:Verdana; font-size: 18px; color: magenta\"> Byte-Pair Encoding (BPE)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abeb0842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['geeksforgeeks', 'is', 'a', 'fantastic', 'resource', 'for', 'geeks', 'who', 'are', 'looking', 'to', 'enhance', 'their', 'programming', 'skills', ',', 'and', 'if', 'you', \"'\", 're', 'geek', 'wants', 'become', 'an', 'expert', 'programmer', 'then', 'definitely', 'the', 'go', '-', 'place', 'like', '.'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "\n",
    "def byte_pair_encoding(vocab):\n",
    "    pairs = get_stats(vocab)\n",
    "    \n",
    "    while len(pairs.values()) != 0:\n",
    "        # Max value\n",
    "        Max = max(list(pairs.values()))\n",
    "\n",
    "        # Find the key(s) that correspond to Max\n",
    "        best_pair = []\n",
    "        for key, value in pairs.items():\n",
    "            if value == Max:\n",
    "                best_pair.append(key)\n",
    "\n",
    "        # Pair the most frequent pairs\n",
    "        for pair in best_pair:\n",
    "            vocab = merge_vocab(pair, vocab)\n",
    "            #print(pair,':',Max)\n",
    "        pairs = get_stats(vocab)\n",
    "    return vocab.keys()\n",
    "    \n",
    "byte_pair_encoding(res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812fb520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
